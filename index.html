<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<link rel="shortcut icon" href="myIcon.ico">
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Cho-Ying Wu, wuchoying, choying wu, Wu choying, Choying, CS, PhD, USC, The University of Southern California"> 
<meta name="description" content="profile">

<link rel="stylesheet" href="jemdoc.css" type="text/css">
<title>Cho-Ying Wu</title>
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1><font face="Arial"> Cho-Ying Wu </font></h1>
				</div>

				<h3><font face="Arial"> Ph.D. </font></h3>
				<p><font face="Arial"> 
					Rm 108, Powell Hall, <br>
					Department of Computer Science, <br>
					University of Southern California, <br>
					Los Angeles, CA. <br>
					<br>
					<em>Email: <a href="mailto:choyingw@usc.edu">choyingw@usc.edu</a></em> <br>
					<a href="supp/CV.pdf"><em>[CV]</em></a>
					<footer class="site-footer">
						<div class="wrapper">
							<div class="footer-col">
							<a href="https://github.com/choyingw" target="_blank">
							<img src="images/color-github.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://twitter.com/ChoYingWu2" target="_blank">
							<img src="images/color-twitter.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://www.linkedin.com/in/cho-ying-wu-4b3b03175/" target="_blank">
							<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
							</a>
							<a href="https://scholar.google.com/citations?user=TTDb6YMAAAAJ&hl" target="_blank">
							<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
							</a>
							</div>
							</div>
						</div>
					</footer>  
				</font></p>
				<!--<p> <a href="https://scholar.google.com/citations?user=tUb4J0kAAAAJ&hl=en"><img src="./pic/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
					<a href="https://github.com/xw-hu"><img src="./pic/github_s.jpg" height="20px" style="margin-bottom:-3px"></a>
					<a href="https://www.facebook.com/xiaowei.hu.102"><img src="./pic/Facebook_s.png" height="30px" style="margin-bottom:-3px"></a>
				</p> -->
			</td>
			<td>
				<img src="images/profile.jpg" border="0" width="240"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>


<p style="text-align:justify";><font face="Arial">
	<p style="color:red;"><b>Currently working at Google Pixel Team for computational photography!</b></p><br>

	I am a fifth-year Ph.D candidate at <strong>CS Department of <a href="https://www.usc.edu/">Univeristy of Southern California</a> </strong> working with Prof. <a href="https://viterbi.usc.edu/directory/faculty/Neumann/Ulrich"><strong>Ulrich Neumann </strong></a>. Before that, I obtained my MS degree in <a href="https://comm.ntu.edu.tw/en/">Graduate Institute of Communication and Engineering</a> at <a href="https://www.ntu.edu.tw/english/">National Taiwan University</a>. I earned a double major degree from National Taiwan University for <a href="https://web.ee.ntu.edu.tw/eng/index.php">Electrial Engineering</a> and <a href="http://www.law.ntu.edu.tw/index.php/eng">Law</a>. <br><br>
    
    I also passed the <a href="http://www.twba.org.tw/en/Report.htm#c02">Attorney of Higher Examination</a> in Taiwan in 2016. This is equalivent to the bar exam in the United States. <br><br>
	
	My research interests are <strong>3D Vision</strong>, <strong>NeRF</strong>, <strong>Generation and Inpainting in 3D</strong>, <strong>Depth Sensing</strong>, <strong>3D Face Modeling</strong>. <br><br>
    
    <p><strong><font face="Arial", color=#519299> Internship:</font></strong></p>

	<style>
		.ImageHolder{
		text-align:center;
		}

		.Image{
		display:inline-block;
		margin-right: 50px;
		margin-bottom: 5px;
		text-align:center;
		}
	</style>

	<div class="Image">
		<img src="images/Argo_AI.png" , height="110">
		<p> <a href="https://www.argo.ai/">Argo AI</a>, 2019</p>
	</div>
	<div class="Image">
		<img src="images/Amazon_126.png" , height="120">
		<p> <a href="https://amazon.jobs/en">Amazon</a>, 2020</p>
	</div>
	<div class="Image">
		<img src="images/facebook.png" , height="120">
		<p> <a href="https://tech.fb.com/ar-vr/">Facebook</a>, 2021</p>
	</div>

	<div class="Image">
		<img src="images/nv.png" , height="120">
		<p> <a href="https://nv-tlabs.github.io/">NVIDIA Toronto AI Lab</a>, 2022</p>
	</div>
    
</font></p>



<!-- <h2><font face="Arial"> News </font></h2>
<ul style="list-style-type:none">
   <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3">
	  [10/2023] 1 papers accepted to CoRL 2023 Workshops. <br> 
	  [03/2023] 1 papers accepted to CVPR 2023 Workshops. <br>  
	  [03/2022] 2 papers accepted to CVPR 2022. <br>
	  [02/2022] I am on the <b>Facebook 2022 PhD Fellowship finalist</b> among ~2200 applicants <br>
	  [09/2021] 1 paper accepted to 3DV 2021 <br>
   	  [01/2021] 1 paper accepted to ICASSP 2021 <br>
   	  [05/2020] 1 paper accepted to CVPR 2020 Workshop on Scalability on Autonomous Driving<br>
   	  [05/2020] 1 paper accepted to CVPR 2020 <br>
   	  [09/2019] 1 paper accepted to GlobalSIP 2019 (Oral) <br>
   	  [09/2019] 1 paper accepted to NeurIPS 2019 <br>
      [05/2019] 1 accepted to ICIP 2019 <br>
      [08/2018] Starting my PhD studies at USC <br>
      [01/2018] 1 paper accepted to Pattern Recognition <br> 
   </font></p>
</ul> -->



<h2><font face="Arial"> Publications </font></h2>
<ul style="list-style-type:none">

	<!-- <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="60%" valign="top"><font color = #246B02> <b>NeRF-sweeper: Inpainting NeRF Road Scenes using 2D Diffusion </b></font><br> 
				<i>  Riccardo de Lutio, <b>Cho-Ying Wu</b>, Huan Ling, Alperen Degirmenci, Janick Martinez Esturo, Sanja Fidler, Or
					Litany, Zan Gojcic</i><br> 
				Under Review <br>
			[<a href="https://arxiv.org/abs/2112.02306">paper</a>]
			[<a href="https://distdepth.github.io/">project page</a>]
			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]
			[<a href="https://distdepth.github.io/">data</a>]
			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]
			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>
			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>
			<p style="color:red;"><i>See our <a href="https://distdepth.github.io/">project page</a> to download the largest datasets for indoor stereo!</i></p>
		</p> <br></td>
			</tr>
	</tbody></table>   -->
	
	
	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/InSpaceType/data_sample.jpeg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>InSpaceType: Reconsider Space Type in Indoor Monocular Depth Estimation </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Quankai Gao, Chin-Cheng Hsu, Te-Lin Wu, Jing-Wen Chen, Ulrich Neumann</i><br> 
				Conference on Robot Learning (<b>CoRL</b>) OOD Workshop, 2023 <br>
			[<a href="https://arxiv.org/abs/2309.13516">paper</a>]
			[<a href="https://depthcomputation.github.io/DepthPublic/">project page</a>]
			[<a href="https://github.com/DepthComputation/InSpaceType_Benchmark">code</a>]
			[<a href="https://depthcomputation.github.io/DepthPublic/">data</a>]<br>
			<i>This work introduces a dataset and benchmark that reconsiders an important but usually overlooked factor- space type. We detailedly analyze ten SOTA models and four popular training dataset and unveil their potential biases.</i>
			<p style="color:red;"><i>See our <a href="https://depthcomputation.github.io/DepthPublic/">project page</a> to download the datasets!</i></p>
		</p> <br></td>
			</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/nerf/teaser.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>NeRF-sweeper: Inpainting NeRF Road Scenes using 2D Diffusion </b></font><br> 
				<i>  Riccardo de Lutio, Cho-Ying Wu, Huan Ling, Alperen Degirmenci, Janick Martinez Esturo, Sanja Fidler, Or Litany, Zan Gojcic</i><br> 
				Intern Project and Under Patent, 2023 <br>
			<br></td>
			</tr>
	</tbody></table>
	
	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/Meta/teaser3.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Meta-Optimization for Higher Model Generalizability in Single-Image Depth Prediction </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Yiqi Zhong, Junying Wang, Ulrich Neumann</i><br> 
				IROS 2024<br>
				CVPR 2023 Workshop Adversarial Machine Learning on Computer Vision<br>
				CVPR 2023 Workshop Computer Vision for Mixed Reality<br>
			[<a href="https://arxiv.org/abs/2305.07269">long version paper</a>]
			<!-- [<a href="https://distdepth.github.io/">project page</a>]
			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]
			[<a href="https://distdepth.github.io/">data</a>]
			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]
			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br> --> <br>
			<i>This work studies learning scheme perspective for popular monocular depth estimation. We formulate our meta-learning based method by novel fine-grained task concept to address less affinity issue in single images. We show performance gain by simply changing learning scheme.</i>
			<!-- <p style="color:red;"><i>See our <a href="https://distdepth.github.io/">project page</a> to download the largest datasets for indoor stereo!</i></p> -->
		</p> <br></td>
			</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/move.gif" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Toward Practical Monocular Indoor Depth Estimation </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Jialiang Wang, Michael Hall, Ulrich Neumann, Shuochen Su</i><br> 
				IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <br>
			[<a href="https://arxiv.org/abs/2112.02306">paper</a>]
			[<a href="https://distdepth.github.io/">project page</a>]
			[<a href="https://github.com/facebookresearch/DistDepth">code</a>]
			[<a href="https://distdepth.github.io/">data</a>]
			[<a href="https://youtu.be/s9JdoR1xbz8">video</a>]
			[<a href="works/DistDepth/CVPR22_DistDepth_poster.pdf">poster</a>]<br>
			<i>Practical indoor depth estimation: without depth annotation, efficient training data collection, high generalizability, and accurate and real-time depth sensing.</i>
			<p style="color:red;"><i>See our <a href="https://distdepth.github.io/">project page</a> to download the largest datasets for indoor stereo!</i></p>
		</p> <br></td>
			</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/Voice2Mesh/img/overall_purpose.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02> <b>Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices? </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Chin-Cheng Hsu, Ulrich Neumann</i><br> 
				IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <br>
			[<a href="https://arxiv.org/abs/2203.09824">paper</a>]
			[<a href="https://github.com/choyingw/Voice2Mesh">code</a>]
			[<a href="works/Voice2Mesh/index.html">project page</a>]
			[<a href="https://youtu.be/0PSyXbLw3oo">video</a>]
			[<a href="works/Voice2Mesh/CVPR22_CMP_poster.pdf">poster</a>]<br>
			<!-- [<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">Youtube video</a>]<br> -->
			<i>An anlaysis on the statistical correlation between voices and 3D faces. Unlike previous work using 2D representations that include background or hairstyle variations, our 3D approach better validate correlation between voices and geometry.</i> 
			<p style="color:red;"><i>See our <a href="works/Voice2Mesh/index.html">project page</a> for explanation of correlation between face geometry and voice!</i></p>
			</p><br></td>
			</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="40%" valign="top">
			<img src="works/SynergyNet/img/teaser.png" style="border-style: none" width="100%">
		</td>
		<td width="60%" valign="top"><font color = #246B02><b>Synergy between 3DMM and 3D Landmarks for Accurate 3D Facial Geometry </b></font><br> 
			<i>  <b>Cho-Ying Wu</b>, Qiangeng Xu, Ulrich Neumann</i><br> 
			  IEEE International Conference on 3D vision (<b>3DV</b>), 2021 <br>
		[<a href="https://arxiv.org/abs/2104.08403">paper</a>]
		[<a href="https://github.com/choyingw/SynergyNet">code</a>]
		[<a href="works/SynergyNet/index.html">project page</a>]
		[<a href="https://youtu.be/i1Y8U2Z20ko">video</a>]
		[<a href="works/SynergyNet/img/2152-Poster.pdf">poster</a>]<br>
		<i>This work attains the <b>state of the art</b> on 3D facial geometry prediction, including 3D facial alignment, face orientation estimation, and 3D face modeling.</i> 
		<p style="color:red;"><i>Check our <a href="https://github.com/choyingw/SynergyNet">code</a> for the SOTA performance 3D facial alignment and face pose estimation!</i></p>
		</p><br></td>
		</tr>
	</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/SCADC/img/demo.gif" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Scene Completeness-Aware Lidar Depth Completion for Driving Scenario </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Ulrich Neumann</i><br> 
				  IEEE International Conference on Acoustics, Speech, & Signal Processing (<b>ICASSP</b>), 2021 <br>
			[<a href="https://arxiv.org/abs/2003.06945">paper</a>]
			[<a href="https://github.com/choyingw/SCADC-DepthCompletion">code</a>]
			[<a href="works/SCADC/index.html">project page</a>]
			[<a href="https://www.youtube.com/watch?v=FQDTdpMPKxs">1-min demo</a>]
			[<a href="https://www.youtube.com/watch?v=IentdAL0Quk">long version video</a>]
			[<a href="works/SCADC/img/2152-Poster.pdf">poster</a>]
			[<a href="works/SCADC/img/2152-slides.pdf">slides</a>]<br>
			<i>This work is the first to attend scene-completeness issue of depth completion. We obtain both structured and accurate scene depth.</i> 
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/GAIS-Net/WSAD/figure/3D2D_v11.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Geometry-Aware Instance Segmentation with Disparity Maps </b></font><br> 
				<i>  <b>Cho-Ying Wu</b>, Xiaoyan Hu, Michael Happold, Qiangeng Xu, Ulrich Neumann</i><br> 
				  IEEE Conference on Computer Vision and Pattern Recognition Workshop Scalability in Autonomous Driving (<b>CVPRw</b>), 2020 <br>
			[<a href="http://www-scf.usc.edu/~choyingw/works/GAIS-Net/WSAD/CVPRW_CameraReady.pdf">paper</a>]
			[<a href="works/GAIS-Net/index.html">project page</a>]
			[<a href="https://github.com/choyingw/GAIS-Net">code</a>]
			[<a href="https://www.youtube.com/watch?v=QmzGAStQXOc">video</a>]<br>
			<i> The first outdoor instacne segmentation that using disparity maps. Based on Mask-RCNN, we show that using multi-modality of geometric information can improve the performance.</i> 
			</p> <br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/others/gridGCN.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Grid-GCN for Fast and Scalable Point Cloud Learning </b></font><br> 
				<i>  Qiangeng Xu, Xudong Sun, <b>Cho-Ying Wu</b>, Panqu Wang, Ulrich Neumann</i><br> 
				  IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020 <br>
			[<a href="https://arxiv.org/abs/1912.02984">paper</a>]
			[<a href="https://github.com/ICpachong/Grid-GCN">code</a>]
			</p><br></td>
			</tr>
		</tbody></table>
 
	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/CFCNet/demo2.gif" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Deep RGB-D Canonical Correlation Analysis for Sparse Depth Completion </b></font><br> 
				<i>  <b>Cho-Ying Wu*</b>, Yiqi Zhong*, Suya You, Ulrich Neumann (*Equal Contribution)</i><br> 
				  Neural Information Processing System (<b>NeurIPS</b>), 2019 <br>
			[<a href="https://arxiv.org/abs/1906.08967">paper</a>]
			[<a href="https://github.com/choyingw/CFCNet">code</a>]
			[<a href="https://www.youtube.com/watch?v=6HCWipHkv60">Youtube video</a>]
			[<a href="works/CFCNet/poster.pdf">poster</a>]<br>
			<i> We study deep canonical correlation analysis for multi-modal fusion on depth completion and attain the SOTA performance when only few sparse measurements are available.</i> 
			</p><br></td>
			</tr>
		</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
	<tbody><tr>
		<td width="40%" valign="top">
			<img src="works/Building_Outline/demo.png" style="border-style: none" width="100%">
		</td>
		<td width="60%" valign="top"><font color = #246B02><b>Salient Building Outline Enhancement and Extraction Using Iterative L0 Smoothing and Line Enhancing </b></font><br> 
			<i>  <b>Cho-Ying Wu</b>, Ulrich Neumann</i><br> 
			  IEEE International Conference on Image Processing (<b>ICIP</b>), 2019 <br>
		[<a href="https://arxiv.org/abs/1906.02426">paper</a>]
		[<a href="https://github.com/choyingw/BuildingOutline">code</a>]
		[<a href="works/Building_Outline/All_GT.zip">groundtruth dataset</a>] 
		[<a href="works/Building_Outline/extra.html">additional results</a>]<br>
		<i> Using iterative operation of L0-smoothing and enhancing, we can extract robust outlines for buildings.</i> 
   
		</p><br></td>
		</tr>
	</tbody></table>

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/others/MDDL.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Efficient Multi-Domain Dictionary Learning with GANs</b></font><br> 
				<i>   <b>Cho-Ying Wu</b>, Ulrich Neumann </i><br> 
				IEEE Global Signal and Information Processing, (<b>GlobalSIP</b>), 2019 (Oral) <br>
			[<a href="https://arxiv.org/abs/1811.00274">paper</a>]<br>
			<i> This work learns multi-domain dictionary from GANs that improve the robustness of dictionary learning.</i>
			</p><br></td>
			</tr>
		</tbody></table>
	
	<!--
	<li> <p style="margin-left: 0px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
            Advanced Optimization Approach for Low-Rank Models with Nonconvex Surrogates and Dual Momentum <br> 
         <i>   <b>Cho Ying Wu</b>, Jian Jiun Ding </i><br> 
	     <br>
	 [paper]
	 </font>
	 </p> </li>
	-->

	<table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/Face_GDHASLR/demo.jpg" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Occluded Face Recognition Using Low-rank Regression with Generalized Gradient Direction</b></font><br> 
				<i>   <b>Cho-Ying Wu</b>, Jian Jiun Ding </i><br> 
						Pattern Recognition (<b>PR</b>), vol. 80, pp. 256–268, 2018<br>
				[<a href="https://www.sciencedirect.com/science/article/pii/S0031320318301079">paper</a>]
				[<a href="works/Face_GDHASLR/Face_GDHASLR.zip">code</a>]<br>
				<i> A robust and efficient occluded face recognition framework that attains the SOTA, using the sparse and low-rank model.</i> 
				</p><br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/others/apsipa17.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>A Fast Non-convex And Non-smooth Regularizer For Low Rank Matrix Completion</b></font><br> 
				<i>   <b>Cho-Ying Wu</b>, Jian-Jiun Ding </i><br> 
						Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (<b>APSIPA-ASC</b>), 2017. <br>
				[<a href="https://ieeexplore.ieee.org/document/8282052">paper</a>]
				</p><br></td>
			</tr>
		</tbody></table>

	 <table cellpadding="10" width="100%" align="center" border="0">
		<tbody><tr>
			<td width="40%" valign="top">
				<img src="works/others/icme16.png" style="border-style: none" width="100%">
			</td>
			<td width="60%" valign="top"><font color = #246B02><b>Occlusion Pattern-based Dictionary For Robust Face Recognition</b></font><br> 
				<i>   <b>Cho-Ying Wu</b>, Jian-Jiun Ding </i><br> 
						IEEE International Conference on Multimedia & Expo (<b>ICME</b>), 2016. <br>
				[<a href="https://ieeexplore.ieee.org/document/7552982">paper</a>]
				</p><br></td>
			</tr>
		</tbody></table>

</ul>


<h2><font face="Arial"> Academic Activities </font></h2>
<ul style="list-style-type:none">
	
	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
              <strong> Reviewer </strong> <br> </font> </p> 
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		 Journal: IEEE Access, Pattern Recognition Letters, Neural Computing<br>
		 Conference: ICIP 2019-2023, CVPR 2022-2024 ECCV 2022-2024, AAAI 2022-2023, NeurIPS 2023, ICCV 2023, ICLR 2024, ICML 2024, ACCV 2024<br>
	      </font> </p> 
         </li>
	 
	 <li> <p style="margin-left: 0px; line-height: 120%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
              <strong> Teaching Assistant </strong> <br> </font> </p> 
	      <p style="margin-left: 20px; line-height: 150%; margin-top: 8px; margin-bottom: 8px;"><font face="Arial" size="3"><meta charset="utf-8">
		   Computer Graphics, University of Southern California, Fall 2022<br>
		   Database Systems, University of Southern California, Spring 2022<br>
		   Computer Graphics, University of Southern California, Fall 2021<br>
		   Database Systems, University of Southern California, Spring 2021<br>
	       Computer Graphics, University of Southern California, Fall 2020<br>
	       Database Systems, University of Southern California, Spring 2020<br>
	       Computer Graphics, University of Southern California, Fall 2019<br>
	       Data Structures and Object Oriented Design, University of Southern California, Spring 2019<br>
	       Advanced Digital Signal Processing, National Taiwan University, Spring 2017 <br>
	       Differential Equation, National Taiwan University, Fall 2016 <br>
	      </font> </p>
	 </li>
	
	
<footer class="site-footer">
<br><br>
<div class="wrapper">
	<div class="footer-col">
	<p>Contacts:</p>
	<a href="https://github.com/choyingw" target="_blank">
	<img src="images/color-github.png" class="social-icon", width="40" height="40">
	</a>



	<a href="https://twitter.com/ChoYingWu2" target="_blank">
	<img src="images/color-twitter.png" class="social-icon", width="40" height="40">
	</a>



	<a href="https://www.linkedin.com/in/cho-ying-wu-4b3b03175/" target="_blank">
	<img src="images/color-linkedin.png" class="social-icon", width="40" height="40">
	</a>



	<a href="https://scholar.google.com/citations?user=TTDb6YMAAAAJ&hl" target="_blank">
	<img src="images/color-gscholar-footer.png" class="social-icon", width="40" height="40">
	</a>


	</div>
	</div>

</div>

</footer>
		
</body></html>

