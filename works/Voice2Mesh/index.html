
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Cross-Modal Perceptionist</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://choyingw.github.io/works/Voice2Mesh/img/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://choyingw.github.io/works/Voice2Mesh/index.html"/>
    <meta property="og:title" content="Cross-Modal Perceptionist" />
    <meta property="og:description" content="Cross-Modal Perceptionist: Cross-Modal 3D Face Model Generation from Voices" />



<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?</br> 
                <small>
                    CVPR 2022
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://choyingw.github.io/">
                          Cho-Ying Wu
                        </a>
                        </br>USC, CGIT Lab
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=5i7k1RoAAAAJ">
                            Chin-Cheng Hsu
                        </a>
                        </br>USC
                    </li>
                    <li>
                        <a href="https://cgit.usc.edu/">
                            Ulrich Neumann
                        </a>
                        </br>USC, CGIT Lab
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2203.09824">
                            <image src="img/paper_image.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/choyingw/Voice2Mesh">
                            <image src="img/github_pad.png" height="60px">
                                <h4><strong>Code and Data</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <center>
                    <iframe width="800" height="450" src="https://www.youtube.com/embed/0PSyXbLw3oo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
                    </iframe>
                </center>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Introduction
                </h3>
                <image src="img/overall_purpose.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    This work digs into a root question in human perception: <b>can face geometry be gleaned from one's voices?</b> 
                    
                    <p>Previous works that study this question only operate on image representation, but inevitably include irrelevant factors, such as hair, cosmetics, and background, which can be changed arbitrarily with the same human voice. Further, correlation between skin color and ethnicity are controversial (See feature visualization from <a href="https://speech2face.github.io/supplemental/index.html#figA">Speech2Face</a>). To clearly study the correlation between voice and geometry, we instead work on 3D, use <b>mesh representation</b>, and focus on geometry only.</p> 
                    <p>We propose two framework to validate the correlation: a supervised method and an unsupervised method. The former is used when pairs of voice and 3D face data exist. The latter is used when we lack such datasets or have low fidelity in them.</p>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Supervised setting 
                </h3>
                <p class="text-justify">
                    The unsupervised framework is shown as follows. This setting serves an ideal case that when paired voice and 3D face data exist. The supervised framework directly learns the 3D face reconstruction pipeline from paired voices and 3D faces.
                </p>
                <p style="text-align:center;">
                    <image src="img/supervised.png" height="50px" class="img-responsive">
                </p>
                <h3>
                    Voxceleb-3D
                </h3>
                <p class="text-justify">
                    We propose a dataset, Voxceleb-3D. We fetch voice banks in Voxceleb and face images from VGGFace for celebrities appear in both datsets. 
                    <p>To obtain 3D mesh, we first extract facial landmarks for images. Then we optimize 3DMM parameters to fit in landmarks. 3DMM is a parametric method to reconstruct meshes by a few controllable parameters. In our work, we use popular BFM faces and use a 62-dim vector to control the face reconstruction. To this end, we obtain paired voice and 3D face data.</p>
                    <p>We show fitting in the following figure, top: images from VGGFace; down: meshes overlayed on images.</p>
                </p>
                <p style="text-align:center;">
                    <image src="img/optimization_synthesis.png" height="50px" class="img-responsive">
                </p>
                <br><br><br><br><br>
                <h3>
                    Unsupervised setting 
                </h3>
                <p class="text-justify">
                    The unsupervised framework is shown as follows. This setting serves a more realistic purpose that it's very hard to obtain large-scale paired voice and 3D face data. The unsupervised framework utilizes the knowledge distillation (KD) to distill knacks from an image-to-3D-face expert to facilitate the unsupervised end-to-end training. Images here is a <b>bridge representation</b> that connects voice and 3D mesh. 
                </p>
                <p style="text-align:center;">
                    <image src="img/unsupervised.png" height="50px" class="img-responsive">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Result:
                </h3>
                <h3>
                    Q1: Is it feasible to predict visually reasonable face meshes from voice?
                </h3>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from supervised learning</p>
                     
                    <image src="img/supervised_gt.png" class="img-responsive" alt="scales">
                </p>
                <br><br><br>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from unsupervised learning</p>
                    <image src="img/many_results_cap.png" class="img-responsive" alt="scales">
                </p>
                <br><br>
                <h3>
                    Q2: How stable is the mesh prediction from different utterances of the same person?
                </h3>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from supervised learning</p>
                     
                    <image src="img/coherence_sup.png" class="img-responsive" alt="scales">
                </p>
                <br><br><br>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from unsupervised learning</p>
                    <image src="img/coherence_unsup.png" class="img-responsive" alt="scales">
                </p>
                <br><br>
                <h3>
                    Q3. Compared with face meshes produced by baselines, can the performance from the joint training flow improve? How much? 
                </h3>
                <p style="text-align:center;">
                    We use the baselines: direct cascaded pretrained models of voice-to-img and img-to-mesh.
                    <image src="img/baseline.png" width=60% class="center" alt="scales">
                </p>
                <br><br>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from supervised learning</p>
                     
                    <image src="img/supervised_comp.png" class="img-responsive" alt="scales">
                </p>
                <br><br><br>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Results from unsupervised learning</p>
                    <image src="img/unsupervised_comp.png" class="img-responsive" alt="scales">
                </p>
                <p style="text-align:center;">
                    <p style="font-size:18px;text-align:center;">Quantitative comparison</p>
                    <image src="img/quantitative_comp.png" width=80% class="center" alt="scales">
                </p>
                <br><br>
                <h3>
                    Q4. What is the major improvement that voice information can bring in the joint training flow?
                </h3>
                <p style="text-align:left;">
                    From the quantitative comparison, we find the major improvement comes from the ear-to-ear ratio (ER), which corresponds to overall wideness of faces. <b>Therefore, the relative face wideness or thinness is the property that voice can indicate.</b> This matches our experience that when someone starts to talk, even before seeing one's face, we can roughly imagine whether one's face is wider or thinner, but we cannot imagine fine-grained details, such as wrinkles or bumps.  
                </p>
                <br><br>
                <h3>
                    Subjective comparison
                </h3>
                <p style="text-align:center;">
                    <image src="img/subjective_comp.png" width=80% class="center" alt="scales">
                </p>
                <br><br>
            </div>
        </div>


            
        <br><br><br>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Impact and Ethics
                </h3>
                <p style="text-align:left;">
                    There are arguably implicit factors, such as voices after smoking and drinking might be different. The data of Voxceleb contains speech from interviews, where interviewees usually speak in normal voices. More implicit and subtle factors such as drug use or health conditions might affect voices, but it needs clinical studies and should be validated from physiological views.
                </p>
                <p style="text-align:left;">
                    The results shown in this work only aim to point out the correlation between voice and face (skull) structure exist and do not make assumptions on race/ethnic origin, and this work does not indicate the relation between race and voice or race and face structure. As mentioned in Introduction, the correlation between race/ethnicity cannot be easily resolved. Besides, the reconstructed meshes do not contain skin color, facial textures, or hairstyles that can explicitly correspond to one’s true identity, and thus anonymity can be preserved.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> 
            </div>
        </div>

    </div>
    
</body>
</html>
